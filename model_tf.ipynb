{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created by Jungsun Yoo on 2020-01-09\n",
    "# define UNet Model in TF\n",
    "import tensorflow as tf\n",
    "\n",
    "def weight_variable(shape, name):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "def bias_variable(shape, name):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides = [1, 2, 2, 1], padding = 'SAME')\n",
    "\n",
    "# tf.nn.conv2d(input, filters, strides, padding, ...)\n",
    "# Given an input tensor of shape [batch, in_height, in_width, in_channels] (=x shape) and a filter\n",
    "# / kernel tensor of shape [filter_height, filter_width, in_channels, out_channels], this op performs the following:\n",
    "# 1. Flattens the filter to a 2D matrix with shape [filter_height * filter_width * in_channels, output_channels]\n",
    "# 2. Extracts image patches from the input tensor to form a virtual tensor of shape [batch, output_height, output_width,\n",
    "# filter_height * filter_width * in_channels]\n",
    "# 3. For each patch, right-multiplies the filter matrix and the image patch vector\n",
    "# Must have strides[0] = strides[3] = 1 \n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1])\n",
    "\n",
    "\n",
    "def upsample_simple(x, [new_height, new_width]):\n",
    "    # it's height, width in TF - not width, height\n",
    "    # new_height = int(round(old_height * scale))\n",
    "    # new_width = int(round(old_width * scale))\n",
    "    # resized = tf.image.resize_images(input_tensor, [new_height, new_width])\n",
    "    return tf.image.resize_images(x, [new_height, new_width])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_tf(x, base, scale):  #base default = 32; scale default = 2\n",
    "    #===== first layer\n",
    "    W_conv1 = weight_variable([3, 3, 1, base], \"W_conv1\") # filter/kernel tensor of shape [filter_height, filter_width, in_channels, out_channels]\n",
    "    b_conv1 = bias_variable([base], \"b_conv1\")\n",
    "\n",
    "    x_image = tf.reshape(x, [-1, 256, 256, 1]) # [batch, in_height, in_width, in_channels]\n",
    "    \n",
    "    h_conv1= tf.nn.relu(conv2d(x_image, W_conv1)+b_conv1)\n",
    "    \n",
    "    W_conv2 = weight_variable([3, 3, base, base], \"W_conv2\")\n",
    "    b_conv2 = bias_variable([base], \"b_conv2\")\n",
    "    \n",
    "    h_conv2 = tf.nn.relu(conv2d(h_conv1, W_conv2) + b_conv2)\n",
    "    \n",
    "    h_pool1 = max_pool_2x2(h_conv2)\n",
    "    \n",
    "    #===== second layer    \n",
    "    W_conv3 = weight_variable([3,3,base, (scale)*base], \"W_conv3\")\n",
    "    b_conv3 = bias_variable([(scale)*base], \"b_conv3\")\n",
    "    \n",
    "    h_conv3 = tf.nn.relu(conv2d(h_pool1, W_conv3) + b_conv3)\n",
    "    \n",
    "    W_conv4 = weight_variable([3,3,(scale)*base,(scale)*base], \"W_conv4\")\n",
    "    b_conv4 = bias_variable([(scale)*base], \"b_conv4\")\n",
    "\n",
    "    h_conv4 = tf.nn.relu(conv2d(h_conv3, W_conv4) + b_conv4)\n",
    "\n",
    "    h_pool2 = max_pool_2x2(h_conv4)\n",
    "    \n",
    "    #===== third layer    \n",
    "    W_conv5 = weight_variable([3,3,(scale)*base, (scale * scale) * base], \"W_conv5\")\n",
    "    b_conv5 = bias_variable([(scale * scale) * base], \"b_conv5\")\n",
    "    \n",
    "    h_conv5 = tf.nn.relu(conv2d(h_pool2, W_conv5) + b_conv5)\n",
    "    \n",
    "    W_conv6 = weight_variable([3,3,(scale * scale) * base,(scale * scale) * base], \"W_conv6\")\n",
    "    b_conv6 = bias_variable([(scale * scale) * base], \"b_conv6\")\n",
    "\n",
    "    h_conv6 = tf.nn.relu(conv2d(h_conv5, W_conv6) + b_conv6)\n",
    "\n",
    "    h_pool3 = max_pool_2x2(h_conv6)   \n",
    "    \n",
    "    #===== fourth layer    \n",
    "    W_convc1 = weight_variable([3,3,(scale * scale)*base, (scale * scale * scale) * base], \"W_convc1\")\n",
    "    b_convc1 = bias_variable([(scale * scale * scale) * base], \"b_convc1\")\n",
    "    \n",
    "    h_convc1 = tf.nn.relu(conv2d(h_pool3, W_convc1) + b_convc1)\n",
    "    \n",
    "    W_convc2 = weight_variable([3,3,(scale * scale * scale) * base,(scale * scale * scale) * base], \"W_convc2\")\n",
    "    b_convc2 = bias_variable([(scale * scale * scale) * base], \"b_convc2\")\n",
    "\n",
    "    h_convc2 = tf.nn.relu(conv2d(h_convc1, W_convc2) + b_convc2)\n",
    "    \n",
    "    #====== fifth layer = start upsampling\n",
    "    # 1. Upsample and merge\n",
    "    \n",
    "    h_pool4 = upsample_simple(h_convc2, [(scale * scale)*base, (scale * scale)*base])\n",
    "    merge = concatenate([h_pool4, h_conv6])\n",
    "    \n",
    "    W_conv7 = weight_variable([3, 3, (scale*scale*scale)*base, (scale*scale)*base], \"W_conv7\")\n",
    "    b_conv7 = bias_variable([(scale*scale)*base], \"b_conv7\")\n",
    "    \n",
    "    h_conv7 = tf.nn.relu(conv2d(merge, W_conv7) + b_conv7)\n",
    "    \n",
    "    W_conv8 = weight_variable([3, 3, (scale*scale)*base, (scale*scale)*base], \"W_conv8\")\n",
    "    b_conv8 = bias_variable([(scale*scale)*base], \"b_conv8\")\n",
    "    \n",
    "    h_conv8 = tf.nn.relu(conv2d(conv7, W_conv8) + b_conv8)\n",
    "    \n",
    "    # ---- sixth layer = upsample2\n",
    "    \n",
    "    h_pool5 = upsample_simple(h_conv8, [(scale)*base, (scale)*base])\n",
    "    merge = concatenate([h_pool5, h_conv4])\n",
    "    \n",
    "    W_conv9 = weight_variable([3, 3, (scale*scale)*base, (scale)*base], \"W_conv9\")\n",
    "    b_conv9 = bias_variable([(scale)*base], \"b_conv9\")\n",
    "    \n",
    "    h_conv9 = tf.nn.relu(conv2d(merge, W_conv9) + b_conv9)\n",
    "    \n",
    "    W_conv10 = weight_variable([3, 3, (scale)*base, (scale)*base], \"W_conv10\")\n",
    "    b_conv10 = bias_variable([(scale)*base], \"b_conv10\")\n",
    "    \n",
    "    h_conv10 = tf.nn.relu(conv2d(h_conv9, W_conv10) + b_conv10)  \n",
    "    \n",
    "    # ------\n",
    "    \n",
    "    h_pool6 = upsample_simple(h_conv10, [base, base])\n",
    "    merge = concatenate([h_pool6, h_conv2])\n",
    "    \n",
    "    W_conv11 = weight_variable([3, 3, (scale)*base, base], \"W_conv11\")\n",
    "    b_conv11 = bias_variable([base], \"b_conv11\")\n",
    "    \n",
    "    h_conv11 = tf.nn.relu(conv2d(merge, W_conv11) + b_conv11)\n",
    "    \n",
    "    W_conv12 = weight_variable([3, 3, base, base], \"W_conv12\")\n",
    "    b_conv12 = bias_variable([base], \"b_conv12\")\n",
    "    \n",
    "    \n",
    "#     out = tf.nn.softmax(conv2d(h_conv11, W_conv12) + b_conv12)\n",
    "    y_conv = tf.nn.softmax(tf.matmul(h_conv11, W_conv12) + b_conv12)\n",
    "#     variable_dict = {\"W_conv1\": W_conv1, \"b_conv1\": b_conv1, \"W_conv2\": W_conv2, \"b_conv2\": b_conv2, \"W_conv3\": W_conv3, \n",
    "#                     \"b_conv3\": b_conv3, }\n",
    "    \n",
    "    return y_conv, variable_dict\n",
    "    \n",
    "#     y_conv = tf.nn.softmax(tf.matmul(merge, W_conv11) + b_conv11)\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = model_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'summary'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-2b10cd56abf7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'summary'"
     ]
    }
   ],
   "source": [
    "a.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
